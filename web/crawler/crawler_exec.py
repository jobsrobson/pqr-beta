"""
**Autor: Victor Kauan**

**Modifica√ß√£o: Robson Ricardo**

M√≥dulo de Coleta de Not√≠cias para o Projeto 'Pergunta que Respondo'.

Este script √© respons√°vel por realizar a coleta automatizada de not√≠cias sobre
educa√ß√£o na Regi√£o Integrada de Desenvolvimento do Distrito Federal e Entorno (RIDE-DF).
Ele utiliza a API da Tavily para buscar links de not√≠cias relevantes e a API Gemini
do Google para extrair o conte√∫do principal de cada p√°gina, limpando-o de
elementos desnecess√°rios como menus, an√∫ncios e rodap√©s.

O processo segue a arquitetura Medallion, salvando os dados brutos extra√≠dos
(camada Bronze) em formato JSON. O script foi projetado para ser executado
diariamente, evitando a duplicidade de coletas atrav√©s de um log de URLs j√°
processadas.

Funcionalidades:

- Busca avan√ßada de not√≠cias com base em uma query.
- Extra√ß√£o de conte√∫do de p√°ginas web com IA (Gemini).
- Valida√ß√£o para descartar p√°ginas que n√£o s√£o artigos de not√≠cia.
- Persist√™ncia dos artigos coletados em arquivos JSON individuais.
- Mecanismo para evitar o reprocessamento de URLs.
- Gerenciamento de chaves de API via vari√°veis de ambiente (.env).

"""

import os
import json
import hashlib
import logging
from datetime import datetime
from pathlib import Path
import pandas as pd
from tavily import TavilyClient
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

FAISS_DATA_PATH = Path("FAISS")
FAISS_DATA_PATH.mkdir(parents=True, exist_ok=True)

# TESTE DE Embeddings locais (sem custo)
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# CONFIGURA√á√ÉO INICIAL
load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# CONFIGURA√á√ÉO DAS APIS
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not TAVILY_API_KEY or not GEMINI_API_KEY:
    raise ValueError("Chaves de API TAVILY_API_KEY ou GEMINI_API_KEY n√£o encontradas no .env")

tavily = TavilyClient(api_key=TAVILY_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)

# CONFIGURA√á√ÉO DO MODELO GEMINI
generation_config = {"temperature": 0.2, "top_p": 1, "top_k": 1, "max_output_tokens": 8192}
safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
]
model = genai.GenerativeModel(model_name="gemini-1.5-flash", generation_config=generation_config, safety_settings=safety_settings)

# CONSTANTES E DIRET√ìRIOS
BRONZE_DATA_PATH = Path("data/bronze")
PROCESSED_URLS_LOG = BRONZE_DATA_PATH / "processed_urls.log"
BRONZE_DATA_PATH.mkdir(parents=True, exist_ok=True)

def carregar_urls_processadas():
    """Carrega o conjunto de URLs j√° processadas do arquivo de log.

    Esta fun√ß√£o l√™ o arquivo 'processed_urls.log' e retorna um conjunto (set)
    contendo todas as URLs que j√° foram coletadas e salvas, para evitar
    reprocessamento. Se o arquivo de log n√£o existir, retorna um conjunto vazio.

    :return: Um conjunto de strings, onde cada string √© uma URL j√° processada.
    :rtype: set[str]
    """
    if not PROCESSED_URLS_LOG.exists():
        return set()
    with open(PROCESSED_URLS_LOG, 'r') as f:
        return set(line.strip() for line in f)

def salvar_url_processada(url):
    """Adiciona uma nova URL ao arquivo de log de URLs processadas.

    Ap√≥s um artigo ser coletado e salvo com sucesso, esta fun√ß√£o √© chamada
    para registrar a URL no arquivo 'processed_urls.log', garantindo que
    ela n√£o seja coletada novamente em execu√ß√µes futuras.

    :param url: A URL que foi processada com sucesso.
    :type url: str
    """
    with open(PROCESSED_URLS_LOG, 'a') as f:
        f.write(f"{url}\n")

def extrair_conteudo_com_ia(url: str, session: requests.Session) -> dict | None:
    """Usa a API Gemini para analisar o HTML de uma URL e extrair o conte√∫do principal.

    Realiza uma requisi√ß√£o GET para a URL, faz uma pr√©-limpeza do HTML com
    BeautifulSoup para remover tags irrelevantes (script, style, etc.), e envia
    o texto resultante para a API Gemini. O prompt instrui o modelo a retornar
    apenas o texto do artigo principal ou uma flag 'NAO_EH_ARTIGO' caso o
    conte√∫do n√£o seja uma not√≠cia v√°lida.

    :param url: A URL da p√°gina da qual o conte√∫do ser√° extra√≠do.
    :type url: str
    :param session: A sess√£o de requests para reutilizar a conex√£o TCP.
    :type session: requests.Session
    :return: Um dicion√°rio contendo o texto extra√≠do na chave 'texto', ou None
             se a extra√ß√£o falhar ou a p√°gina n√£o for um artigo v√°lido.
    :rtype: dict | None
    """
    logging.info(f"Processando com IA: {url}")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    try:
        response = session.get(url, headers=headers, timeout=20)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
            tag.decompose()
        
        html_pre_limpo = soup.get_text(separator='\n', strip=True)

        if len(html_pre_limpo) < 500:
             logging.warning("Descartado: Conte√∫do pr√©-limpeza muito curto.")
             return None

        prompt_parts = [
            "Voc√™ √© um especialista em extra√ß√£o de dados de p√°ginas web.",
            "Analise o seguinte texto extra√≠do de um HTML e retorne APENAS o texto limpo e coeso do artigo principal.",
            "Ignore qualquer texto de menu, an√∫ncios, links de 'leia tamb√©m', avisos de cookies ou rodap√©s.",
            "Se o conte√∫do n√£o parecer um artigo de not√≠cia completo (ex: √© apenas uma lista de links, uma galeria de fotos ou uma p√°gina de erro), retorne EXATAMENTE a palavra 'NAO_EH_ARTIGO'.",
            "\n--- IN√çCIO DO TEXTO DA P√ÅGINA ---\n",
            html_pre_limpo[:15000],
            "\n--- FIM DO TEXTO DA P√ÅGINA ---\n",
            "ARTIGO EXTRA√çDO:",
        ]
        
        response_ia = model.generate_content(prompt_parts)
        texto_extraido = response_ia.text.strip()

        if "NAO_EH_ARTIGO" in texto_extraido or len(texto_extraido) < 250:
            logging.info("Veredito da IA: N√£o √© um artigo v√°lido.")
            return None

        logging.info("Veredito da IA: Artigo extra√≠do com sucesso.")
        return {"texto": texto_extraido}

    except Exception as e:
        logging.error(f"Erro ao processar a URL {url} com IA: {e}")
        return None

def salvar_artigo_em_json(artigo: dict):
    """Salva um dicion√°rio de artigo como um arquivo JSON na camada Bronze.

    O nome do arquivo √© gerado de forma a ser √∫nico e informativo, contendo
    a data da coleta, a fonte da not√≠cia e um hash da URL. O arquivo √© salvo
    no diret√≥rio definido pela constante BRONZE_DATA_PATH.

    :param artigo: Um dicion√°rio contendo os dados do artigo
                   (fonte, titulo, link, texto, etc.).
    :type artigo: dict
    """
    # Gera um hash do link para um nome de arquivo √∫nico e curto
    url_hash = hashlib.md5(artigo['link'].encode()).hexdigest()[:10]
    timestamp = datetime.now().strftime("%Y-%m-%d")
    filename = f"{timestamp}_{artigo['fonte'].replace('.', '_')}_{url_hash}.json"
    filepath = BRONZE_DATA_PATH / filename

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(artigo, f, ensure_ascii=False, indent=4)
    logging.info(f"Artigo salvo em: {filepath}")

def executar_coleta(query: str):
    """Orquestra o processo completo de coleta de not√≠cias.

    Esta √© a fun√ß√£o principal do fluxo de coleta. Ela realiza os seguintes passos:
    1. Busca por uma query na API da Tavily para obter URLs de not√≠cias.
    2. Carrega a lista de URLs j√° processadas para evitar duplicatas.
    3. Itera sobre as URLs encontradas.
    4. Para cada URL nova, chama `extrair_conteudo_com_ia` para obter o texto.
    5. Se a extra√ß√£o for bem-sucedida, monta o dicion√°rio completo do artigo.
    6. Salva o artigo em um arquivo JSON com `salvar_artigo_em_json`.
    7. Registra a URL como processada com `salvar_url_processada`.

    :param query: A string de busca a ser usada para encontrar not√≠cias relevantes.
    :type query: str
    """
    logging.info(f"Buscando no Tavily pela query: '{query}'")
    
    try:
        response_tavily = tavily.search(query=query, search_depth="advanced", max_results=20)
        urls_tavily = response_tavily.get('results', [])
        logging.info(f"Busca conclu√≠da! {len(urls_tavily)} URLs candidatas encontradas.")
    except Exception as e:
        logging.error(f"Falha ao buscar no Tavily: {e}")
        return

    urls_processadas = carregar_urls_processadas()
    artigos_novos_count = 0
    
    with requests.Session() as session:
        for item in urls_tavily:
            url = item.get('url')
            if not url or url in urls_processadas:
                if url in urls_processadas:
                    logging.info(f"URL j√° processada, pulando: {url}")
                continue

            resultado_ia = extrair_conteudo_com_ia(url, session)

            if resultado_ia:
                artigo_completo = {
                    "fonte": item.get('source', url.split('/')[2]),
                    "titulo": item.get('title', 'T√≠tulo n√£o encontrado'),
                    "link": url,
                    "texto": resultado_ia['texto'],
                    "data_coleta": datetime.now().isoformat(),
                    "query_origem": query
                }
                salvar_artigo_em_json(artigo_completo)
                atualizar_faiss(artigo_completo) # ATUALIZA√á√ÉO DO √çNDICE FAISS
                salvar_url_processada(url)
                artigos_novos_count += 1
    
    logging.info(f"Processo finalizado. {artigos_novos_count} novos artigos foram coletados e salvos.")


def atualizar_faiss(artigo: dict):
    """
    Atualiza o √≠ndice FAISS com o artigo rec√©m-coletado.
    Se j√° existir, adiciona; se n√£o, cria um novo.
    """
    texto = f"{artigo['titulo']} - {artigo['texto']}"
    
    if (FAISS_DATA_PATH / "index.faiss").exists():
        # Carregar √≠ndice existente
        vector_store = FAISS.load_local(
            str(FAISS_DATA_PATH), 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        vector_store.add_texts([texto])
        logging.info("üîÑ √çndice FAISS atualizado com nova not√≠cia")
    else:
        # Criar √≠ndice do zero
        vector_store = FAISS.from_texts([texto], embedding=embeddings)
        logging.info("üÜï √çndice FAISS criado do zero")
    
    vector_store.save_local(str(FAISS_DATA_PATH))
    logging.info(f"üíæ √çndice FAISS salvo em {FAISS_DATA_PATH}")


if __name__ == "__main__":
    # EXECU√á√ÉO PARA COLETA HIST√ìRICA
    # Rode este bloco apenas uma vez.
    # [Inference] Ajustando para o ano corrente (2025) para maior relev√¢ncia nos testes.
    meses_ano = [
        "agosto de 2025", "julho de 2025", "junho de 2025", 
        "maio de 2025", "abril de 2025", "mar√ßo de 2025"
    ]

    for periodo in meses_ano:
        query_historica = f"not√≠cias sobre educa√ß√£o no Distrito Federal e RIDE em {periodo}"
        executar_coleta(query_historica)

    # EXECU√á√ÉO PARA COLETA DI√ÅRIA
    # Comente o bloco acima ap√≥s o uso e use este para a automa√ß√£o di√°ria.
    # query_diaria = "not√≠cias recentes sobre educa√ß√£o na Regi√£o Integrada de Desenvolvimento do Distrito Federal e Entorno (RIDE-DF)"
    # executar_coleta(query_diaria)